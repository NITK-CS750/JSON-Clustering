{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, random\n",
    "dir = os.getcwd()\n",
    "datasets = os.path.join(dir, \"..\", \"datasets\")\n",
    "outputs = os.path.join(dir, \"..\", \"outputs\")\n",
    "\n",
    "from SetSimilaritySearch import all_pairs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTL, RTL extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(string, mode, name=None):\n",
    "    json_dict = []\n",
    "    count = 0\n",
    "    if string == \"imdb\":\n",
    "        with open(os.path.join(datasets, string), encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                doc = json.loads(line)\n",
    "                json_dict.append(doc)\n",
    "                count+=1\n",
    "    else:\n",
    "        with open(os.path.join(datasets, string), encoding='utf-8') as f:\n",
    "            json_dict = json.load(f)\n",
    "            count = len(json_dict)\n",
    "\n",
    "    print(count, \"documents loaded.\")\n",
    "\n",
    "    stack = []\n",
    "    final_dict = {}\n",
    "    all_keys = set()\n",
    "\n",
    "    def do_walk(datadict):\n",
    "        if isinstance(datadict, dict):\n",
    "            for key, value in datadict.items():\n",
    "                stack.append(key)\n",
    "                if isinstance(value, dict) and len(value.keys()) == 0:\n",
    "                    for val in stack:\n",
    "                        all_keys.add(val)\n",
    "                    final_dict[\"/\".join(stack)] = \"EMPTY_DICT\"\n",
    "                if isinstance(value, list) and len(value) == 0:\n",
    "                    for val in stack:\n",
    "                        all_keys.add(val)\n",
    "                    final_dict[\"/\".join(stack)] = 'EMPTY_LIST'\n",
    "                if isinstance(value, dict):\n",
    "                    do_walk(value)\n",
    "                if isinstance(value, list):\n",
    "                    do_walk(value)\n",
    "                if isinstance(value, str):\n",
    "                    for val in stack:\n",
    "                        all_keys.add(val)\n",
    "                    final_dict[\"/\".join(stack)] = value\n",
    "                stack.pop()\n",
    "\n",
    "        if isinstance(datadict, list):\n",
    "            n = 0\n",
    "            for key in datadict:\n",
    "                n = n + 1\n",
    "                if isinstance(key, dict):\n",
    "                    do_walk(key)\n",
    "                if isinstance(key, list):\n",
    "                    do_walk(key)\n",
    "                if isinstance(key, str):\n",
    "                    for val in stack:\n",
    "                        all_keys.add(val)\n",
    "                    final_dict[\"/\".join(stack)] = key\n",
    "\n",
    "    keys_list = []\n",
    "    rtl_paths_list = []\n",
    "    for i in range(0,len(json_dict)):\n",
    "        do_walk(json_dict[i])\n",
    "        keys_list.append(all_keys)\n",
    "        rtl_paths_list.append([x for x in final_dict.keys()])\n",
    "        final_dict={}\n",
    "        all_keys=set()\n",
    "\n",
    "    def flatten(t):\n",
    "        return [item for sublist in t for item in sublist]\n",
    "\n",
    "    if mode == \"ntl\":\n",
    "        import re\n",
    "        final_append_array = []\n",
    "        for document in rtl_paths_list:\n",
    "            for path in document:\n",
    "                if path is not None:\n",
    "                    result = [path[_.start()+1:] for _ in re.finditer(\"/\", path)]\n",
    "                for item in result : document.append(item)\n",
    "            final_append_array.append(list(set(document)))\n",
    "        rtl_paths_list = final_append_array\n",
    "\n",
    "    f = open(os.path.join(outputs, name or \"NTL_paths_list.json\"), \"w\")\n",
    "    f.write(json.dumps(rtl_paths_list, indent=4))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13797 documents loaded.\n",
      "23374 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"imdb\", \"ntl\", \"f1\")\n",
    "load_doc(\"Full_Schema.json\", \"ntl\", \"f2\")\n",
    "with open(os.path.join(outputs, \"f1\")) as f1:\n",
    "    with open(os.path.join(outputs, \"f2\")) as f2:\n",
    "        sets1 = json.load(f1)\n",
    "        sets = sets1[0:50]\n",
    "        sets2 = json.load(f2)\n",
    "        sets += sets2[0:50]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"jaccard\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.to_csv(os.path.join(outputs, \"similarity_result.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard on RTL - IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13797 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"imdb\", \"rtl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"jaccard\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"JACCARD SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.to_csv(os.path.join(outputs, \"JACCARD-IMDB-RTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard on NTL - IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13797 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"imdb\", \"ntl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"jaccard\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"JACCARD SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.dropna().to_csv(os.path.join(outputs, \"JACCARD-IMDB-NTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard on RTL - Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23374 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"Full_Schema.json\", \"rtl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"jaccard\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"JACCARD SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.dropna().to_csv(os.path.join(outputs, \"JACCARD-Synthetic-RTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard on NTL - Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23374 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"Full_Schema.json\", \"ntl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"jaccard\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"JACCARD SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.dropna().to_csv(os.path.join(outputs, \"JACCARD-Synthetic-NTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine on RTL - IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13797 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"imdb\", \"rtl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"cosine\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"COSINE SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.dropna().to_csv(os.path.join(outputs, \"COSINE-IMDB-RTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine on NTL - IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13797 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"imdb\", \"ntl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"cosine\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"COSINE SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.dropna().to_csv(os.path.join(outputs, \"COSINE-IMDB-NTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine on RTL - Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23374 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"Full_Schema.json\", \"rtl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"cosine\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"COSINE SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.dropna().to_csv(os.path.join(outputs, \"COSINE-Synthetic-RTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine on NTL - Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23374 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"Full_Schema.json\", \"ntl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"cosine\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"COSINE SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.dropna().to_csv(os.path.join(outputs, \"COSINE-Synthetic-NTL.csv\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fbd45983fa86a8ec9cc8767573693f6dfd80dec01b23ff7ec08ec74cce86369"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
