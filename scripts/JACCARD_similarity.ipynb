{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/ashueep/JSON-Clustering/scripts/JACCARD_similarity.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashueep/JSON-Clustering/scripts/JACCARD_similarity.ipynb#ch0000000?line=3'>4</a>\u001b[0m outputs \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ashueep/JSON-Clustering/scripts/JACCARD_similarity.ipynb#ch0000000?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mSetSimilaritySearch\u001b[39;00m \u001b[39mimport\u001b[39;00m all_pairs\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ashueep/JSON-Clustering/scripts/JACCARD_similarity.ipynb#ch0000000?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import json, os, random\n",
    "dir = os.getcwd()\n",
    "datasets = os.path.join(dir, \"..\", \"datasets\")\n",
    "outputs = os.path.join(dir, \"..\", \"outputs\")\n",
    "\n",
    "from SetSimilaritySearch import all_pairs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(string, mode):\n",
    "    json_dict = []\n",
    "    count = 0\n",
    "    if string == \"imdb\":\n",
    "        with open(os.path.join(datasets, string), encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                doc = json.loads(line)\n",
    "                json_dict.append(doc)\n",
    "                count+=1\n",
    "    else:\n",
    "        with open(os.path.join(datasets, string), encoding='utf-8') as f:\n",
    "            json_dict = json.load(f)\n",
    "            count = len(json_dict)\n",
    "\n",
    "    print(count, \"documents loaded.\")\n",
    "\n",
    "    stack = []\n",
    "    final_dict = {}\n",
    "    all_keys = set()\n",
    "\n",
    "    def do_walk(datadict):\n",
    "        if isinstance(datadict, dict):\n",
    "            for key, value in datadict.items():\n",
    "                stack.append(key)\n",
    "                if isinstance(value, dict) and len(value.keys()) == 0:\n",
    "                    for val in stack:\n",
    "                        all_keys.add(val)\n",
    "                    final_dict[\"/\".join(stack)] = \"EMPTY_DICT\"\n",
    "                if isinstance(value, list) and len(value) == 0:\n",
    "                    for val in stack:\n",
    "                        all_keys.add(val)\n",
    "                    final_dict[\"/\".join(stack)] = 'EMPTY_LIST'\n",
    "                if isinstance(value, dict):\n",
    "                    do_walk(value)\n",
    "                if isinstance(value, list):\n",
    "                    do_walk(value)\n",
    "                if isinstance(value, str):\n",
    "                    for val in stack:\n",
    "                        all_keys.add(val)\n",
    "                    final_dict[\"/\".join(stack)] = value\n",
    "                stack.pop()\n",
    "\n",
    "        if isinstance(datadict, list):\n",
    "            n = 0\n",
    "            for key in datadict:\n",
    "                n = n + 1\n",
    "                if isinstance(key, dict):\n",
    "                    do_walk(key)\n",
    "                if isinstance(key, list):\n",
    "                    do_walk(key)\n",
    "                if isinstance(key, str):\n",
    "                    for val in stack:\n",
    "                        all_keys.add(val)\n",
    "                    final_dict[\"/\".join(stack)] = key\n",
    "\n",
    "    keys_list = []\n",
    "    rtl_paths_list = []\n",
    "    for i in range(0,len(json_dict)):\n",
    "        do_walk(json_dict[i])\n",
    "        keys_list.append(all_keys)\n",
    "        rtl_paths_list.append([x for x in final_dict.keys()])\n",
    "        final_dict={}\n",
    "        all_keys=set()\n",
    "\n",
    "    def flatten(t):\n",
    "        return [item for sublist in t for item in sublist]\n",
    "\n",
    "    if mode == \"ntl\":\n",
    "        import re\n",
    "        final_append_array = []\n",
    "        for document in rtl_paths_list:\n",
    "            for path in document:\n",
    "                if path is not None:\n",
    "                    result = [path[_.start()+1:] for _ in re.finditer(\"/\", path)]\n",
    "                for item in result : document.append(item)\n",
    "            final_append_array.append(list(set(document)))\n",
    "        rtl_paths_list = final_append_array\n",
    "\n",
    "    f = open(os.path.join(outputs, \"NTL_paths_list.json\"), \"w\")\n",
    "    f.write(json.dumps(rtl_paths_list, indent=4))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard on RTL - IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13797 documents loaded.\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"imdb\", \"rtl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"jaccard\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"JACCARD SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.to_csv(os.path.join(outputs, \"JACCARD-IMDB-RTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard on NTL - IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13797 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"imdb\", \"ntl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"jaccard\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"JACCARD SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.to_csv(os.path.join(outputs, \"JACCARD-IMDB-NTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard on RTL - Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23374 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"Full_Schema.json\", \"rtl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"jaccard\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"JACCARD SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.to_csv(os.path.join(outputs, \"JACCARD-Synthetic-RTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard on NTL - Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23374 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"Full_Schema.json\", \"ntl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"jaccard\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"JACCARD SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.to_csv(os.path.join(outputs, \"JACCARD-Synthetic-NTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine on RTL - IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13797 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"imdb\", \"rtl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"cosine\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"COSINE SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.to_csv(os.path.join(outputs, \"COSINE-IMDB-RTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine on NTL - IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13797 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"imdb\", \"ntl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"cosine\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"COSINE SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.to_csv(os.path.join(outputs, \"COSINE-IMDB-NTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine on RTL - Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23374 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"Full_Schema.json\", \"rtl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"cosine\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"COSINE SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.to_csv(os.path.join(outputs, \"COSINE-Synthetic-RTL.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine on NTL - Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23374 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "load_doc(\"Full_Schema.json\", \"ntl\")\n",
    "with open(os.path.join(outputs, \"NTL_paths_list.json\")) as f:\n",
    "    sets = json.load(f)\n",
    "    sets = sets[0:500]\n",
    "\n",
    "pairs = all_pairs(sets, similarity_func_name = \"cosine\", similarity_threshold = 0)\n",
    "data = list(pairs)\n",
    "df = pd.DataFrame(data, columns=[\"Document 1\", \"Document 2\", \"COSINE SIMILARITY\"]).sort_values(by = [\"Document 2\", \"Document 1\"])\n",
    "df.to_csv(os.path.join(outputs, \"COSINE-Synthetic-NTL.csv\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fbd45983fa86a8ec9cc8767573693f6dfd80dec01b23ff7ec08ec74cce86369"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
