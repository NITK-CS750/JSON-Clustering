{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "GEA7113vaHUO",
      "metadata": {
        "id": "GEA7113vaHUO"
      },
      "source": [
        "# Contextual and Semantic Similarity\n",
        "\n",
        "This notebook contains code using to obtain the semantic and contextual similarity scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SaI5hRg7aHUT",
      "metadata": {
        "id": "SaI5hRg7aHUT"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json, os\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "import math\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LuCj7AWxaHUW",
      "metadata": {
        "id": "LuCj7AWxaHUW"
      },
      "source": [
        "## Loading the dataset to train on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b25a72ba",
      "metadata": {
        "id": "b25a72ba"
      },
      "outputs": [],
      "source": [
        "# loading imdb and synthetic dataset\n",
        "dir = os.path.dirname(\"__file__\")\n",
        "datasets = os.path.join(dir, \"..\", \"datasets\")\n",
        "outputs = os.path.join(dir, \"..\", \"outputs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4DzXNfv8aHUY",
      "metadata": {
        "id": "4DzXNfv8aHUY"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "syn1_dict = []\n",
        "syn2_dict = []\n",
        "syn3_dict = []\n",
        "\n",
        "count = 0\n",
        "with open(os.path.join(datasets, \"imdb\"), encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        doc = json.loads(line)\n",
        "        dataset.append(doc)\n",
        "        count+=1\n",
        "\n",
        "# Synthetic_2 contains the first 10000-odd documents from the synthetic dataset supplied to us\n",
        "with open(os.path.join(datasets, \"Synthetic_2.json\"), encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        doc = json.loads(line)\n",
        "        syn1_dict.append(doc)\n",
        "\n",
        "print(count)\n",
        "\n",
        "with open(os.path.join(datasets, \"Full_Schema_1.json\"), encoding='utf-8') as f:\n",
        "    json_data = f.read()\n",
        "    syn2_dict = json.loads(json_data)\n",
        "\n",
        "with open(os.path.join(datasets, \"Full_Schema.json\"), encoding='utf-8') as f:\n",
        "    json_data = f.read()\n",
        "    syn3_dict = json.loads(json_data)\n",
        "\n",
        "dataset.extend(syn1_dict)\n",
        "dataset.extend(syn2_dict)\n",
        "dataset.extend(syn3_dict)\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_02zAza_aHUZ",
      "metadata": {
        "id": "_02zAza_aHUZ",
        "outputId": "8554503f-701c-4cdf-efc2-56b54e4d2580"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['id/$oid',\n",
              " 'book/doi',\n",
              " 'booktitle',\n",
              " 'author',\n",
              " 'book/booktitle',\n",
              " '$oid',\n",
              " 'doi',\n",
              " 'book/author']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# finding RTL and NTL paths list\n",
        "stack = []\n",
        "final_dict = {}\n",
        "all_keys = set()\n",
        "\n",
        "def do_walk(datadict):\n",
        "    if isinstance(datadict, dict):\n",
        "        for key, value in datadict.items():\n",
        "            stack.append(key)\n",
        "            if isinstance(value, dict) and len(value.keys()) == 0:\n",
        "                for val in stack:\n",
        "                    all_keys.add(val)\n",
        "                final_dict[\"/\".join(stack)] = \"EMPTY_DICT\"\n",
        "            if isinstance(value, list) and len(value) == 0:\n",
        "                for val in stack:\n",
        "                    all_keys.add(val)\n",
        "                final_dict[\"/\".join(stack)] = 'EMPTY_LIST'\n",
        "            if isinstance(value, dict):\n",
        "                do_walk(value)\n",
        "            if isinstance(value, list):\n",
        "                do_walk(value)\n",
        "            if isinstance(value, str):\n",
        "                for val in stack:\n",
        "                    all_keys.add(val)\n",
        "                final_dict[\"/\".join(stack)] = value\n",
        "            stack.pop()\n",
        "\n",
        "    if isinstance(datadict, list):\n",
        "        n = 0\n",
        "        for key in datadict:\n",
        "            n = n + 1\n",
        "            if isinstance(key, dict):\n",
        "                do_walk(key)\n",
        "            if isinstance(key, list):\n",
        "                do_walk(key)\n",
        "            if isinstance(key, str):\n",
        "                for val in stack:\n",
        "                    all_keys.add(val)\n",
        "                final_dict[\"/\".join(stack)] = key\n",
        "\n",
        "keys_list = []\n",
        "ntl_paths_list_util = []\n",
        "rtl_paths_list = []\n",
        "for i in range(0,len(dataset)):\n",
        "    do_walk(dataset[i])\n",
        "    keys_list.append(all_keys)\n",
        "    ntl_paths_list_util.append([x for x in final_dict.keys()])\n",
        "    rtl_paths_list.append(list(final_dict.keys()))\n",
        "    final_dict={}\n",
        "    all_keys=set()\n",
        "\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "\n",
        "final_append_array = []\n",
        "for document in ntl_paths_list_util:\n",
        "    for path in document:\n",
        "        if path is not None:\n",
        "            result = [path[_.start()+1:] for _ in re.finditer(\"/\", path)]\n",
        "        for item in result : document.append(item)\n",
        "    final_append_array.append(list(set(document)))\n",
        "ntl_paths_list = final_append_array\n",
        "\n",
        "(ntl_paths_list[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1hxyu03oaHUb",
      "metadata": {
        "id": "1hxyu03oaHUb",
        "outputId": "2b61652f-3b8c-4036-9487-9eb9749b5409"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['id/$oid',\n",
              "  'article/author',\n",
              "  'article/journal',\n",
              "  'article/title',\n",
              "  'article/doi'],\n",
              " ['id/$oid',\n",
              "  'article/author',\n",
              "  'article/journal',\n",
              "  'article/title',\n",
              "  'article/doi'],\n",
              " ['id/$oid',\n",
              "  'article/author',\n",
              "  'article/journal',\n",
              "  'article/title',\n",
              "  'article/doi']]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rtl_paths_list[13797:13800]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rxVXMBGLaHUc",
      "metadata": {
        "id": "rxVXMBGLaHUc",
        "outputId": "9c430c04-5a9c-456a-859f-7be48ceb60a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['id $oid', 'article author journal title doi'],\n",
              " ['id $oid', 'article author journal title doi'],\n",
              " ['id $oid', 'article author journal title doi']]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# preparing sentences to be fed into bert\n",
        "sent_list = []\n",
        "for path in rtl_paths_list:\n",
        "    temp_list = []\n",
        "    i = 0\n",
        "    while i in range(len(path)):\n",
        "        if \"/\" not in path[i]:\n",
        "            temp_list.append(path[i])\n",
        "            i += 1\n",
        "        else:\n",
        "            left_str = path[i][:path[i].find(\"/\")]\n",
        "            temp_list2 = []\n",
        "            temp_list2.append(path[i][path[i].find(\"/\")+1:])\n",
        "            i += 1\n",
        "            while i in range(len(path)) and left_str in path[i]:\n",
        "                temp_list2.append(path[i][path[i].find(\"/\")+1:])\n",
        "                i += 1\n",
        "            temp_str = \"\"\n",
        "            if len(temp_list2) == 1 and temp_list2[0] not in left_str:\n",
        "                temp_str = left_str + \" \" + temp_list2[0]\n",
        "            elif len(temp_list2) == 1:\n",
        "                temp_str = left_str\n",
        "            else:\n",
        "                temp_str = left_str + \" \" + ' '.join(temp_list2)\n",
        "            temp_list.append(temp_str)\n",
        "    sent_list.append(temp_list)\n",
        "\n",
        "sent_list[13797:13800]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077eae2e",
      "metadata": {
        "id": "077eae2e"
      },
      "outputs": [],
      "source": [
        "# getting models ready\n",
        "\n",
        "bert_model = SentenceTransformer('C:/Users/desik/.cache/torch/sentence_transformers/sentence-transformers_bert-base-nli-mean-tokens')\n",
        "# roberta_model = SentenceTransformer('C:/Users/desik/.cache/torch/sentence_transformers/sentence-transformers_all-roberta-large-v1')\n",
        "# electra_model = SentenceTransformer('C:/Users/desik/.cache/torch/sentence_transformers/ddobokki_electra-small-nli-sts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b94006fd",
      "metadata": {
        "id": "b94006fd"
      },
      "outputs": [],
      "source": [
        "# to get meaningful words out of the labels\n",
        "\n",
        "def viterbi_segment(text):\n",
        "    probs, lasts = [1.0], [0]\n",
        "    for i in range(1, len(text) + 1):\n",
        "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
        "                        for j in range(max(0, i - max_word_length), i))\n",
        "        probs.append(prob_k)\n",
        "        lasts.append(k)\n",
        "    words = []\n",
        "    i = len(text)\n",
        "    while 0 < i:\n",
        "        words.append(text[lasts[i]:i])\n",
        "        i = lasts[i]\n",
        "    words.reverse()\n",
        "    return words, probs[-1]\n",
        "\n",
        "def word_prob(word): return dictionary[word] / total\n",
        "def words(text): return re.findall('[a-z]+', text.lower()) \n",
        "dictionary = Counter(words(open('../datasets/big.txt').read()))\n",
        "max_word_length = max(map(len, dictionary))\n",
        "total = float(sum(dictionary.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NyfhDMiDaHUf",
      "metadata": {
        "id": "NyfhDMiDaHUf"
      },
      "source": [
        "## Contextual similarity with just the RTL paths of the JSON documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c784e0f9",
      "metadata": {
        "id": "c784e0f9"
      },
      "outputs": [],
      "source": [
        "# formatting keys to more meaningful words\n",
        "def string_replace_meaningful(key_list):\n",
        "    for k in range(len(key_list)):\n",
        "        str_list = key_list[k].split()\n",
        "        for i in range(len(str_list)):\n",
        "            str1 = str_list[i].replace('cinematgrs', 'cinematographers').replace('costdes', 'costumedes').replace('aka', 'alternate')\n",
        "            str1 = str1.replace('proddesi', 'productiondesi').replace('doi', 'digitalidentifier').replace('publtype', 'publishingtype').replace('ref', 'reference')\n",
        "            str1 = str1.replace(\"_id\", '').replace(\"$oid\", '').replace('_key', '').replace('imdb', 'moviedatabase').replace('url', '')\n",
        "            word_split = viterbi_segment(str1)\n",
        "            str1 = ' '.join(word_split[0])\n",
        "            str_list[i] = str1\n",
        "\n",
        "        key_list[k] = \" \".join(str_list)\n",
        "\n",
        "        if len(str_list) == 1:\n",
        "            key_list[k] = str_list[0]\n",
        "        elif len(str_list) == 2:\n",
        "            key_list[k] = \" \".join(str_list)\n",
        "        elif len(str_list) > 2:\n",
        "            left = str_list[0]\n",
        "            str2 = \" \".join(str_list[1:len(str_list)-1])  \n",
        "            key_list[k] = left + \" \" + str2 + \" and \" + str_list[-1]    \n",
        "    return key_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0274a51d",
      "metadata": {
        "id": "0274a51d",
        "outputId": "501cf53d-5bdb-4f23-a695-46b599287301"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Document has id , article author journal title and digital identifier'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# converting RTL paths to sentence\n",
        "key_list_sentences = []\n",
        "for i in range(len(sent_list)):\n",
        "    t_list = sent_list[i][:]\n",
        "    t_list = string_replace_meaningful(t_list)\n",
        "    # if (i < 3): print(t_list)\n",
        "    sent = ', '.join(t_list)\n",
        "    sent = \"Document has \" + sent\n",
        "    key_list_sentences.append(sent)\n",
        "\n",
        "(key_list_sentences[13797])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OMU11DPpaHUh",
      "metadata": {
        "id": "OMU11DPpaHUh"
      },
      "outputs": [],
      "source": [
        "# to return bert embeddings\n",
        "import random\n",
        "def embeddings(sentence_list):\n",
        "    # embeddings = model.encode(sentence_list)\n",
        "    all_docs = bert_model.encode(sentence_list)\n",
        "    print(\"done.\")\n",
        "    return all_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BNNWjC0iaHUh",
      "metadata": {
        "id": "BNNWjC0iaHUh",
        "outputId": "73631d2e-3211-4a91-d1da-123b63358a95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['identifier'], 9.047317470370035e-07)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "viterbi_segment(\"identifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gNbHt1SoaHUi",
      "metadata": {
        "id": "gNbHt1SoaHUi"
      },
      "outputs": [],
      "source": [
        "# bert_scores = similarity_scores(key_list_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IeJG6AZiaHUi",
      "metadata": {
        "id": "IeJG6AZiaHUi"
      },
      "source": [
        "## Contextual similarity with Content of JSON documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B2qV_x7jaHUi",
      "metadata": {
        "id": "B2qV_x7jaHUi",
        "outputId": "9eee969e-16d4-4a6f-d859-d67383b093ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\" title has 'Adam 12' (1990) (Real Estate Scam (#1.8)) actors has  sex akaname name   directors has  name   producers has  name   writers has  name   releasedates has   country  USA imdbdate  12 November 1990 releasedate  1990-11-12 addition  null \",\n",
              " \" title has 'Adam 12' (1990) (Teach the Children (#1.9)) actors has  sex akaname name   directors has  name   producers has  name   writers has  name   releasedates has   country  USA imdbdate  19 November 1990 releasedate  1990-11-19 addition  null \",\n",
              " \" title has 'Adam 12' (1990) (The Landlord (#1.14)) actors has  sex akaname name   directors has  name   producers has  name   writers has  name   releasedates has   country  USA imdbdate  24 December 1990 releasedate  1990-12-24 addition  null \",\n",
              " \" title has 'Adam 12' (1990) (The Sniper (#1.1)) actors has  sex akaname name   directors has  name   producers has  name   writers has  name   releasedates has   country  USA imdbdate  24 September 1990 releasedate  1990-09-24 addition  null \",\n",
              " \" title has 'Adam 12' (1990) (Trick or Trick (#2.4)) year has null actors has  sex akaname name  \",\n",
              " \" title has 'Adam 12' (1990) (Vigilante (#1.6)) actors has  sex akaname name   directors has  name   producers has  name   writers has  name   releasedates has   country  USA imdbdate  29 October 1990 releasedate  1990-10-29 addition  null \",\n",
              " \" title has 'Adam 12' (1990) (Witchcraft (#1.5)) actors has  sex akaname name   directors has  name   producers has  name   writers has  name   releasedates has   country  USA imdbdate  22 October 1990 releasedate  1990-10-22 addition  null \",\n",
              " \" title has 'Adam Adamant Lives!' (1966) year has 1966-1967 directors has  name   proddesigners has   name  Evans, Gwen (I) London, Ray Middleton, Malcolm Hulke, Sally Ruddy, Austin Rae, Mary  writers has  name   plots has   plottext  Edwardian adventurer Adam Adamant is frozen alive in a block of ice by his arch-nemesis The Face in 1902 ; in 1966 workmen discover him and he is revived, perfectly preserved... but completely bewildered by his new environment, 'swinging 60's' London, until he meets up with the beautiful Georgina Jones, who helps him adapt - and before long, he is back to adventuring, solving crime & fighting evil wherever it may lurk Anonymous  ratings has   rank  6.6 votes  34 distribution  0.0.000402  releasedates has   country  UK imdbdate  3 June 1966 releasedate  1966-06-03 addition  null  runningtimes has   time  50 addition  (29 episodes)  language has   language  English  keywords has   keyword  hero-from-the-past character-name-in-title  genres has   genre  Adventure Sci-Fi  distributors has   name  Just Entertainment [nl] - (2007) (Netherlands) (DVD) (17 episodes)  countries has   country  UK  colorinfo has   color  Black and White \",\n",
              " \" title has 'Adam Adamant Lives!' (1966) (A Sinister Sort of Service (#2.13)) year has 1967 actors has  sex akaname name   directors has  name   proddesigners has   name  Young, Michael (III)  producers has  name   writers has  name   releasedates has   country  UK imdbdate  25 March 1967 releasedate  1967-03-25 addition  null \"]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# converting JSON document into sentences\n",
        "def convert(doc) -> str:\n",
        "    s = ''\n",
        "    i = 0\n",
        "    for (key, value) in doc.items():\n",
        "        has = ''\n",
        "        if key in ['movieid', 'crossref', '_id', '_key', 'ee', 'url', 'pages', 'number', 'pages', 'imdburl']: # ignoring key-value pairs that do not bring any contextual meaning\n",
        "            continue\n",
        "        if type(value) == type(dict()):\n",
        "            has = convert(value)\n",
        "            s += f' {key} has {has} '\n",
        "        elif type(value) == type(list()):\n",
        "            has = convert_list(key, value)\n",
        "            s += f' {key} has {has} '\n",
        "        else:\n",
        "            s += f\" {key} has \" + str(value)\n",
        "            continue\n",
        "        i = i+1\n",
        "        \n",
        "\n",
        "    return s\n",
        "\n",
        "def convert_list(key_v, values):\n",
        "    dict_ret = {}\n",
        "    if type(values[0]) != dict:\n",
        "        sent = ' '.join(values)\n",
        "        return sent\n",
        "\n",
        "    for key, value in values[0].items():\n",
        "        dict_ret[key] = []\n",
        "\n",
        "    sent = ' '\n",
        "\n",
        "    if key_v in ['actors', 'directors', 'producers', 'cinematgrs', 'costdesigners', 'misc', 'editors', 'composers', 'writers']:\n",
        "        temp_set = set()\n",
        "        for i in values:\n",
        "            for key, value in i.items():\n",
        "                temp_set.add(key)\n",
        "        \n",
        "        for i in temp_set:\n",
        "            sent = sent + i + \" \"\n",
        "\n",
        "\n",
        "    else:\n",
        "        for item in values:\n",
        "            for key, value in item.items():\n",
        "                \n",
        "                if type(value) == list:\n",
        "                    value = \", \".join(value)\n",
        "                if key in dict_ret.keys():\n",
        "                    dict_ret[key].append(value)\n",
        "                else:\n",
        "                    temp_list = []\n",
        "                    temp_list.append(value)\n",
        "                    dict_ret[key] = temp_list\n",
        "\n",
        "\n",
        "        for key, value in dict_ret.items():\n",
        "            sent = sent + \" \" + key + \" \"\n",
        "            sent_t = \" \".join(value)\n",
        "            sent = sent + \" \" + sent_t\n",
        "\n",
        "\n",
        "    return sent\n",
        "\n",
        "sentences = []\n",
        "\n",
        "for i in range(13797):\n",
        "    sentences.append(convert(dataset[i]))\n",
        "\n",
        "sentences[-10:-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MqTTNISIaHUj",
      "metadata": {
        "id": "MqTTNISIaHUj",
        "outputId": "16aab35a-ae59-4396-9ff6-37bfdb71b4a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13797\n"
          ]
        }
      ],
      "source": [
        "# function to clean the sentences for BERT\n",
        "import unicodedata\n",
        "def clean_sentence(val):\n",
        "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
        "    regex = re.compile('([^\\s\\w]|_)+')\n",
        "    sentence = regex.sub('', val)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
        "\n",
        "def string_replace(sentence):\n",
        "    for k in range(len(sentence)):\n",
        "        str_list = sentence[k].split()\n",
        "        for i in range(len(str_list)):\n",
        "            str1 = str_list[i].replace('cinematgrs', 'cinematographers').replace('costdes', 'costume des').replace('akan', 'alternate n')\n",
        "            str1 = str1.replace('proddesi', 'production desi').replace('doi', 'digital identifier').replace('publtype', 'publishing type').replace('ref', 'reference')\n",
        "            str1 = str1.replace(\"_id\", '').replace(\"$oid\", '').replace('_key', '').replace('imdb', 'movie database ').replace('url', '')\n",
        "            str_list[i] = str1\n",
        "\n",
        "        sentence[k] = \" \".join(str_list)\n",
        "  \n",
        "    return sentence\n",
        "\n",
        "print(len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-2sO6MTmaHUj",
      "metadata": {
        "id": "-2sO6MTmaHUj",
        "outputId": "31d188da-f9e7-49f0-e22b-b72eb2bc3684"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13797"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for i in range(len(sentences)):\n",
        "    sentences[i] = clean_sentence(sentences[i])\n",
        "    sentences[i] = remove_accents(sentences[i])\n",
        "    \n",
        "sentences_f = string_replace(sentences)\n",
        "\n",
        "len(sentences_f)\n",
        "\n",
        "# with open('sentences_taken.txt', 'w', encoding=\"utf-8\") as f:\n",
        "#     f.write('\\n'.join(sentences_f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i5UqjnKyaHUk",
      "metadata": {
        "id": "i5UqjnKyaHUk",
        "outputId": "d6bc93fd-42bc-4da7-fae1-f1e5fce5cfbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Document has id , article author journal title and digital identifier'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "key_list_sentences[13799]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9iUovYJ5aHUk",
      "metadata": {
        "id": "9iUovYJ5aHUk"
      },
      "outputs": [],
      "source": [
        "# finding similarity scores from BERT\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "sentences_f.extend(key_list_sentences[13797:30000])\n",
        "# sentences_f[6900:7100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pNGyCvCpaHUl",
      "metadata": {
        "id": "pNGyCvCpaHUl",
        "outputId": "a3ad0177-946b-456f-b651-b8d693402876"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sentences_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MzKg4BD5aHUl",
      "metadata": {
        "id": "MzKg4BD5aHUl",
        "outputId": "77d4f540-05fa-48a3-f934-b673d6336194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_scores = embeddings(sentences_f)\n",
        "len(bert_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fy8w1Tr0do3B",
      "metadata": {
        "id": "fy8w1Tr0do3B"
      },
      "source": [
        "### Clustering with just the contextual scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yiXhv-hpaHUl",
      "metadata": {
        "id": "yiXhv-hpaHUl"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fgs2x00aHUm",
      "metadata": {
        "id": "6fgs2x00aHUm"
      },
      "outputs": [],
      "source": [
        "cluster_model = AgglomerativeClustering(n_clusters=2, affinity='cosine', linkage='average')\n",
        "cluster_model.fit(bert_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vFteb_u6aHUm",
      "metadata": {
        "id": "vFteb_u6aHUm",
        "outputId": "a55aa0af-7e0d-436f-c554-b6bc6c02a5ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([    0,     1,     2, ..., 13794, 13795, 13796], dtype=int64),)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.where(cluster_model.labels_ == 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RBhBzCW_aHUm",
      "metadata": {
        "id": "RBhBzCW_aHUm"
      },
      "source": [
        "## Semantic Similarity with Wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PdifskmSaHUn",
      "metadata": {
        "id": "PdifskmSaHUn"
      },
      "outputs": [],
      "source": [
        "# using Michael Lesk algorithm for word sense disambiguation\n",
        "class Lesk(object):\n",
        "\n",
        "    def __init__(self, sentence):\n",
        "        self.sentence = sentence\n",
        "        self.meanings = {}\n",
        "        for word in sentence:\n",
        "            self.meanings[word] = ''\n",
        "\n",
        "    def getSenses(self, word):\n",
        "        # print word\n",
        "        return wn.synsets(word.lower())\n",
        "\n",
        "    def getGloss(self, senses):\n",
        "\n",
        "        gloss = {}\n",
        "\n",
        "        for sense in senses:\n",
        "            gloss[sense.name()] = []\n",
        "\n",
        "        for sense in senses:\n",
        "            gloss[sense.name()] += word_tokenize(sense.definition())\n",
        "\n",
        "        return gloss\n",
        "\n",
        "    def getAll(self, word):\n",
        "        senses = self.getSenses(word)\n",
        "\n",
        "        if senses == []:\n",
        "            return {word.lower(): senses}\n",
        "\n",
        "        return self.getGloss(senses)\n",
        "\n",
        "    def Score(self, set1, set2):\n",
        "        # Base\n",
        "        overlap = 0\n",
        "\n",
        "        # Step\n",
        "        for word in set1:\n",
        "            if word in set2:\n",
        "                overlap += 1\n",
        "\n",
        "        return overlap\n",
        "\n",
        "    def overlapScore(self, word1, word2):\n",
        "\n",
        "        gloss_set1 = self.getAll(word1)\n",
        "        if self.meanings[word2] == '':\n",
        "            gloss_set2 = self.getAll(word2)\n",
        "        else:\n",
        "            # print 'here'\n",
        "            gloss_set2 = self.getGloss([wn.synset(self.meanings[word2])])\n",
        "\n",
        "        # print gloss_set2\n",
        "\n",
        "        score = {}\n",
        "        for i in gloss_set1.keys():\n",
        "            score[i] = 0\n",
        "            for j in gloss_set2.keys():\n",
        "                score[i] += self.Score(gloss_set1[i], gloss_set2[j])\n",
        "\n",
        "        bestSense = None\n",
        "        max_score = 0\n",
        "        for i in gloss_set1.keys():\n",
        "            if score[i] > max_score:\n",
        "                max_score = score[i]\n",
        "                bestSense = i\n",
        "\n",
        "        return bestSense, max_score\n",
        "\n",
        "    def lesk(self, word, sentence):\n",
        "        maxOverlap = 0\n",
        "        context = sentence\n",
        "        word_sense = []\n",
        "        meaning = {}\n",
        "\n",
        "        senses = self.getSenses(word)\n",
        "\n",
        "        for sense in senses:\n",
        "            meaning[sense.name()] = 0\n",
        "\n",
        "        for word_context in context:\n",
        "            if not word == word_context:\n",
        "                score = self.overlapScore(word, word_context)\n",
        "                if score[0] == None:\n",
        "                    continue\n",
        "                meaning[score[0]] += score[1]\n",
        "\n",
        "        if senses == []:\n",
        "            return word, None, None\n",
        "\n",
        "        self.meanings[word] = max(meaning.keys(), key=lambda x: meaning[x])\n",
        "\n",
        "        return word, self.meanings[word], wn.synset(self.meanings[word]).definition()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F2et1vbeaHUn",
      "metadata": {
        "id": "F2et1vbeaHUn"
      },
      "outputs": [],
      "source": [
        "from scipy import spatial\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "def path(set1, set2):\n",
        "    return wn.path_similarity(set1, set2)\n",
        "\n",
        "\n",
        "def wup(set1, set2):\n",
        "    return wn.wup_similarity(set1, set2)\n",
        "\n",
        "\n",
        "def edit(word1, word2):\n",
        "    if float(edit_distance(word1, word2)) == 0.0:\n",
        "        return 0.0\n",
        "    return 1.0 / float(edit_distance(word1, word2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3IQoZnb-aHUn",
      "metadata": {
        "id": "3IQoZnb-aHUn"
      },
      "outputs": [],
      "source": [
        "def tokenize(q1):\n",
        "    return word_tokenize(q1)\n",
        "\n",
        "\n",
        "def posTag(q1):\n",
        "    return nltk.pos_tag(q1)\n",
        "\n",
        "\n",
        "def stemmer(tag_q1):\n",
        "    stem_q1 = []\n",
        "\n",
        "    for token in tag_q1:\n",
        "        stem_q1.append(stem(token))\n",
        "\n",
        "    return stem_q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8n7K9dxyaHUo",
      "metadata": {
        "id": "8n7K9dxyaHUo"
      },
      "outputs": [],
      "source": [
        "def computePath(q1, q2):\n",
        "\n",
        "    R = np.zeros((len(q1), len(q2)))\n",
        "\n",
        "    for i in range(len(q1)):\n",
        "        for j in range(len(q2)):\n",
        "            if q1[i][1] == None or q2[j][1] == None:\n",
        "                sim = edit(q1[i][0], q2[j][0]) # using edit distance\n",
        "            else:\n",
        "                sim = path(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
        "\n",
        "            if sim == None:\n",
        "                sim = edit(q1[i][0], q2[j][0])\n",
        "\n",
        "            R[i, j] = sim\n",
        "\n",
        "    # print R\n",
        "\n",
        "    return R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KxklVSugaHUo",
      "metadata": {
        "id": "KxklVSugaHUo"
      },
      "outputs": [],
      "source": [
        "def computeWup(q1, q2):\n",
        "\n",
        "    R = np.zeros((len(q1), len(q2)))\n",
        "\n",
        "    for i in range(len(q1)):\n",
        "        for j in range(len(q2)):\n",
        "            if q1[i][1] == None or q2[j][1] == None:\n",
        "                sim = edit(q1[i][0], q2[j][0])\n",
        "            else:\n",
        "                sim = wup(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
        "\n",
        "            if sim == None:\n",
        "                sim = edit(q1[i][0], q2[j][0])\n",
        "\n",
        "            R[i, j] = sim\n",
        "\n",
        "    # print R\n",
        "\n",
        "    return R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LNPAq1RiaHUo",
      "metadata": {
        "id": "LNPAq1RiaHUo"
      },
      "outputs": [],
      "source": [
        "def overallSim(q1, q2, R):\n",
        "\n",
        "    sum_X = 0.0\n",
        "    sum_Y = 0.0\n",
        "\n",
        "    for i in range(len(q1)):\n",
        "        max_i = 0.0\n",
        "        for j in range(len(q2)):\n",
        "            if R[i, j] > max_i:\n",
        "                max_i = R[i, j]\n",
        "        sum_X += max_i\n",
        "\n",
        "    for i in range(len(q1)):\n",
        "        max_j = 0.0\n",
        "        for j in range(len(q2)):\n",
        "            if R[i, j] > max_j:\n",
        "                max_j = R[i, j]\n",
        "        sum_Y += max_j\n",
        "        \n",
        "    if (float(len(q1)) + float(len(q2))) == 0.0:\n",
        "        return 0.0\n",
        "        \n",
        "    overall = (sum_X + sum_Y) / (2 * (float(len(q1)) + float(len(q2))))\n",
        "\n",
        "    return overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eR8vg_RXaHUo",
      "metadata": {
        "id": "eR8vg_RXaHUo"
      },
      "outputs": [],
      "source": [
        "STOP_WORDS = nltk.corpus.stopwords.words()\n",
        "def clean_sentence_nltk(val):\n",
        "    #\"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
        "    regex = re.compile('([^\\s\\w]|_)+')\n",
        "    sentence = regex.sub('', val).lower()\n",
        "    sentence = sentence.split(\" \")\n",
        "\n",
        "    for word in list(sentence):\n",
        "        if word in STOP_WORDS:\n",
        "            sentence.remove(word)\n",
        "\n",
        "    sentence = \" \".join(sentence)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m39aKzpqaHUp",
      "metadata": {
        "id": "m39aKzpqaHUp"
      },
      "outputs": [],
      "source": [
        "def sentenceProcessing(s1):\n",
        "    token1 = tokenize(s1)\n",
        "    tag1 = posTag(token1)\n",
        "    sentence = []\n",
        "\n",
        "    for i, word in enumerate(tag1):\n",
        "        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
        "            sentence.append(word[0])\n",
        "\n",
        "    sense1 = Lesk(sentence)\n",
        "    sentence1Means = []\n",
        "    for word in sentence:\n",
        "        sentence1Means.append(sense1.lesk(word, sentence))\n",
        "\n",
        "    return sentence1Means\n",
        "\n",
        "def semanticSimilarity(s1, s2):\n",
        "    R1 = computePath(s1, s2)\n",
        "    R2 = computeWup(s1, s2)\n",
        "\n",
        "    R = (R1 + R2) / 2\n",
        "    # print R\n",
        "    return overallSim(s1, s2, R)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uZl37fhCaHUp",
      "metadata": {
        "id": "uZl37fhCaHUp"
      },
      "outputs": [],
      "source": [
        "def prepareSent(s1):\n",
        "    temp_list = sentenceProcessing(s1)\n",
        "    return temp_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vvs8kqKQaHUr",
      "metadata": {
        "id": "vvs8kqKQaHUr"
      },
      "outputs": [],
      "source": [
        "inputs = sentences_f[0:10000]\n",
        "answer = []\n",
        "for i in range(len(inputs)):\n",
        "    answer.append(prepareSent(inputs[i]))\n",
        "    if i%100 == 0:\n",
        "        print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O2RZoL__aHUr",
      "metadata": {
        "id": "O2RZoL__aHUr"
      },
      "outputs": [],
      "source": [
        "inputs = sentences_f[10000:]\n",
        "for i in range(len(inputs)):\n",
        "    answer.append(prepareSent(inputs[i]))\n",
        "    if i%100 == 0:\n",
        "        print(i)\n",
        "\n",
        "# note: this and the above cell will take a long time to run, given the fact that WordNet goes through individual words and finds the best fit for it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "icnGPWSWaHUr",
      "metadata": {
        "id": "icnGPWSWaHUr"
      },
      "outputs": [],
      "source": [
        "# writing all the outputs into files\n",
        "\n",
        "final_list = []     # compiling the BERT embeddings, wordnet embeddings and the NTL paths into one\n",
        "\n",
        "for i in range(len(bert_scores)):\n",
        "    temp_list = []\n",
        "    temp_list.append(i)\n",
        "    temp_list.append(bert_scores[i].tolist())\n",
        "    temp_list.append(answer[i])\n",
        "    temp_list.append(ntl_paths_list[i])\n",
        "    final_list.append(temp_list)\n",
        "\n",
        "\n",
        "with open(os.path.join(outputs, \"scores_list.json\"), 'w') as f:\n",
        "    # print(json.dumps(dicts))\n",
        "    f.write(json.dumps(final_list))\n",
        "    f.write('\\n')\n",
        "\n",
        "with open(os.path.join(outputs, \"dataset_f.json\"), 'w') as f:\n",
        "    f.write(json.dumps(dataset[0:30000]))\n",
        "    f.write('\\n')\n",
        "\n",
        "with open(os.path.join(outputs, \"sentences.json\"), 'w') as f:\n",
        "    f.write(json.dumps(sentences_f))\n",
        "    f.write('\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "contextual_semantic_similarity.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "9c5a8d1ae379d35a3ef6f4635f13f61b80f963fd96ced72459a9887c17ad40e6"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
